/*
 * cosmos_wrapper.c - Wrapper to compile scx_cosmos as userspace C
 *
 * This file includes the simulator wrapper infrastructure and then
 * the actual scheduler source. The header guards in common.bpf.h
 * prevent re-inclusion, so our overridden macros take effect.
 *
 * NOTE: This file is compiled with -Dconst= to strip const qualifiers.
 * BPF schedulers declare globals as "const volatile" (patched by the
 * BPF loader). Stripping const makes them writable from Rust.
 */
#include "sim_wrapper.h"
#include "sim_task.h"

/*
 * COSMOS-specific macro overrides (defined after sim_wrapper.h,
 * before main.bpf.c).
 */

/*
 * Enable scx_bpf_select_cpu_and — implemented in the simulator.
 * With flat_idle_scan=false, COSMOS will use this instead of flat scan.
 */
#undef bpf_ksym_exists
#define bpf_ksym_exists(sym) (1)

/*
 * The simulator always calls select_cpu before enqueue, so the
 * CPU is always selected.
 */
#undef __COMPAT_is_enq_cpu_selected
#define __COMPAT_is_enq_cpu_selected(enq_flags) (true)

/*
 * Simulated task_struct doesn't have migration_disabled field;
 * bpf_core_field_exists override would make the real function
 * try to access it.
 */
#undef is_migration_disabled
#define is_migration_disabled(p) ((void)(p), false)

/*
 * Route bpf_map_lookup_percpu_elem to a static cpu_ctx array.
 * Forward-declared here; defined after the scheduler source since
 * struct cpu_ctx is defined there.
 */
#define MAX_SIM_CPUS 128
static struct cpu_ctx *cosmos_lookup_percpu_elem(int cpu);
#undef bpf_map_lookup_percpu_elem
#define bpf_map_lookup_percpu_elem(map, key, cpu) cosmos_lookup_percpu_elem(cpu)

/*
 * Simulated perf counters.
 *
 * Use scx_bpf_now() as a monotonic counter. start_counters() records
 * the baseline at task start; stop_counters() reads the current value
 * at task stop. Delta = task runtime in nanoseconds.
 *
 * Tasks with runtime > perf_threshold are classified as "event heavy"
 * and routed to the least-busy-event CPU.
 */
extern u64 scx_bpf_now(void);

static long sim_perf_event_read(void *map, u32 key,
				struct bpf_perf_event_value *val, u32 size)
{
	(void)map; (void)key; (void)size;
	val->counter = scx_bpf_now();
	val->enabled = 1;
	val->running = 1;
	return 0;
}
#undef bpf_perf_event_read_value
#define bpf_perf_event_read_value(map, key, val, size) \
	sim_perf_event_read(map, key, val, size)

/*
 * Per-CPU start_readings storage.
 *
 * The BPF start_readings map is PERCPU_ARRAY — each CPU needs its own
 * baseline. Override bpf_map_lookup_elem to route start_readings lookups
 * to this array; all other maps fall through to scx_test_map_lookup_elem.
 *
 * start_readings_map_ptr is set in cosmos_register_maps() after
 * main.bpf.c is included (where start_readings is defined).
 */
static struct bpf_perf_event_value start_readings_percpu[MAX_SIM_CPUS];
static void *start_readings_map_ptr;

static void *cosmos_map_lookup(void *map, const void *key)
{
	if (map == start_readings_map_ptr && start_readings_map_ptr != NULL) {
		int cpu = bpf_get_smp_processor_id();
		if (cpu >= 0 && cpu < MAX_SIM_CPUS)
			return &start_readings_percpu[cpu];
		return NULL;
	}
	return scx_test_map_lookup_elem(map, key);
}
#undef bpf_map_lookup_elem
#define bpf_map_lookup_elem(map, key) cosmos_map_lookup((void *)(map), key)

/*
 * Include COSMOS interface header, then the scheduler source.
 * common.bpf.h is already included (header guard set), so our
 * BPF_STRUCT_OPS and SCX_OPS_DEFINE overrides are in effect.
 *
 * We include a patched copy of main.bpf.c (generated by config.mk)
 * that guards against division-by-zero in update_freq(). BPF
 * division-by-zero returns 0; native C crashes with SIGFPE.
 */
#include "intf.h"
#include "cosmos_main_patched.c"

/*
 * Static per-CPU context array, defined after the scheduler source
 * so that struct cpu_ctx is available.
 */
static struct cpu_ctx percpu_ctx[MAX_SIM_CPUS];

static struct cpu_ctx *cosmos_lookup_percpu_elem(int cpu)
{
	if (cpu < 0 || cpu >= MAX_SIM_CPUS)
		return NULL;
	return &percpu_ctx[cpu];
}

/*
 * Register the COSMOS BPF maps with the test map infrastructure.
 *
 * Both task_ctx_stor (TASK_STORAGE) and cpu_node_map (HASH) are
 * registered here; cpu_ctx_stor (PERCPU_ARRAY) is handled by the
 * static array above.
 */
static struct scx_test_map task_ctx_map;
static struct scx_test_map cpu_node_test_map;

void cosmos_register_maps(void)
{
	scx_test_map_clear_all();

	INIT_SCX_TEST_MAP_FROM_TASK_STORAGE(&task_ctx_map, task_ctx_stor);
	scx_test_map_register(&task_ctx_map, &task_ctx_stor);

	INIT_SCX_TEST_MAP(&cpu_node_test_map, cpu_node_map);
	scx_test_map_register(&cpu_node_test_map, &cpu_node_map);

	start_readings_map_ptr = (void *)&start_readings;
}

/*
 * Combined setup function called from Rust before cosmos_init().
 * Sets global variables to disable complex features, registers maps,
 * and enables CPU 0 in the primary domain.
 */
void cosmos_setup(unsigned int num_cpus)
{
	struct cpu_arg arg = { .cpu_id = 0 };

	smt_enabled = true;
	avoid_smt = true;
	primary_all = true;
	flat_idle_scan = false;
	preferred_idle_scan = false;
	cpufreq_enabled = true;
	numa_enabled = false;
	nr_node_ids = 1;
	mm_affinity = false;
	perf_enabled = true;
	deferred_wakeups = false;
	slice_ns = 20000000;   /* 20ms */
	slice_lag = 20000000;  /* 20ms */
	busy_threshold = 1;   /* system "not busy" → flat idle scan path */

	cosmos_register_maps();
	enable_primary_cpu(&arg);
}

/*
 * Configure NUMA topology after setup.
 * Populates cpu_node_map with sequential grouping:
 * CPUs [0, cpus_per_node) → node 0, etc.
 * Enables NUMA-aware scheduling in COSMOS.
 */
void cosmos_configure_numa(unsigned int num_cpus, unsigned int nr_nodes)
{
	unsigned int cpus_per_node, cpu, node;

	if (nr_nodes <= 1)
		return;  /* leave numa_enabled=false */

	cpus_per_node = num_cpus / nr_nodes;
	for (cpu = 0; cpu < num_cpus; cpu++) {
		node = cpu / cpus_per_node;
		if (node >= nr_nodes)
			node = nr_nodes - 1;
		bpf_map_update_elem(&cpu_node_map, &cpu, &node, 0);
	}

	numa_enabled = true;
	nr_node_ids = nr_nodes;
}
