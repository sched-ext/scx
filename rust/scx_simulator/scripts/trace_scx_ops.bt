#!/usr/bin/env bpftrace
/*
 * trace_scx_ops.bt - Trace sched_ext ops callbacks and kfunc calls
 *
 * Two layers of tracing:
 *   1. sched_class kprobes  - mark when ops callbacks execute
 *   2. scx_bpf_* kfunc fexit - what the BPF scheduler does inside
 *
 * Design notes:
 *   - fentry on sched_ext_ops__* stubs does NOT fire when a BPF scheduler
 *     is loaded (trampolines bypass them).
 *   - sched_class functions are LTO-mangled on this kernel, so we use
 *     wildcard kprobes (e.g. select_task_rq_scx*) to match regardless of
 *     the LTO hash suffix. Struct dereferences in kprobe entries cause BPF
 *     complexity failures when combined with many probes, so entries use
 *     builtins only. PIDs come from kfuncs instead.
 *   - kretprobes are omitted: bpftrace v0.24 doesn't support retval with
 *     wildcard probes, and the kfunc fexits already provide return values.
 *
 * Usage:
 *   # Default: trace CPUs 0-3
 *   sudo bpftrace scripts/trace_scx_ops.bt > /tmp/scx_trace.log 2>&1
 *
 *   # Custom CPU range: trace CPUs 0-7
 *   sudo bpftrace scripts/trace_scx_ops.bt 8 > /tmp/scx_trace.log 2>&1
 *
 *   # All CPUs (noisy!):
 *   sudo bpftrace scripts/trace_scx_ops.bt 1024 > /tmp/scx_trace.log 2>&1
 *
 * Output format:
 *   TIMESTAMP_NS cpu=N >> EVENT                   (sched_class entry)
 *   TIMESTAMP_NS cpu=N .. EVENT key=value...      (kfunc call + return)
 *   TIMESTAMP_NS cpu=N == EVENT key=value...      (lifecycle tracepoint)
 *
 * Mapping from sched_class -> ops callbacks:
 *   select_task_rq  -> ops.select_cpu(p, prev_cpu, wake_flags)
 *   enqueue_task    -> ops.runnable(p) then ops.enqueue(p, flags)
 *   dequeue_task    -> ops.quiescent(p) or ops.dequeue(p)
 *   balance         -> ops.dispatch(cpu, prev)
 *   set_next_task   -> ops.running(p)
 *   put_prev_task   -> ops.stopping(p, runnable)
 *   task_tick       -> ops.tick(p)
 */

BEGIN {
    /* Default to 4 CPUs; override with: bpftrace script.bt <nr_cpus> */
    @max_cpu = $1 > 0 ? $1 : 4;
    printf("Tracing sched_ext ops + kfuncs on CPUs 0-%d. Ctrl-C to stop.\n\n",
        @max_cpu - 1);
}

/* ================================================================
 * SCHED_CLASS ENTRY POINTS (kprobe only)
 *
 * Wildcard kprobes match regardless of LTO hash suffix.
 * No struct dereference to avoid BPF complexity limits.
 * PIDs and return values come from kfunc fexits instead.
 * ================================================================ */

/* select_task_rq: called to pick a CPU for a waking task */
kprobe:select_task_rq_scx*
/ cpu < @max_cpu / {
    printf("%llu cpu=%d >> select_task_rq\n", nsecs, cpu);
}

/* enqueue_task: calls ops.runnable then ops.enqueue */
kprobe:enqueue_task_scx*
/ cpu < @max_cpu / {
    printf("%llu cpu=%d >> enqueue_task\n", nsecs, cpu);
}

/* dequeue_task: calls ops.quiescent or ops.dequeue */
kprobe:dequeue_task_scx*
/ cpu < @max_cpu / {
    printf("%llu cpu=%d >> dequeue_task\n", nsecs, cpu);
}

/* balance: calls ops.dispatch to fill local DSQ */
kprobe:balance_scx*
/ cpu < @max_cpu / {
    printf("%llu cpu=%d >> balance\n", nsecs, cpu);
}

/* set_next_task: calls ops.running - task starts executing */
kprobe:set_next_task_scx*
/ cpu < @max_cpu / {
    printf("%llu cpu=%d >> set_next_task\n", nsecs, cpu);
}

/* put_prev_task: calls ops.stopping - task stops executing */
kprobe:put_prev_task_scx*
/ cpu < @max_cpu / {
    printf("%llu cpu=%d >> put_prev_task\n", nsecs, cpu);
}

/* task_tick: calls ops.tick - periodic scheduler tick */
kprobe:task_tick_scx*
/ cpu < @max_cpu / {
    printf("%llu cpu=%d >> task_tick\n", nsecs, cpu);
}

/* ================================================================
 * KFUNC CALLS (fexit - captures args + return values via BTF)
 *
 * Note: kfunc names vary by kernel version. This kernel uses
 * scx_bpf_dispatch (not scx_bpf_dsq_insert) and scx_bpf_consume
 * (not scx_bpf_dsq_move_to_local).
 * ================================================================ */

fexit:scx_bpf_select_cpu_dfl
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. select_cpu_dfl pid=%d prev_cpu=%d ret=%d\n",
        nsecs, cpu, args.p->pid, args.prev_cpu, retval);
}

fexit:scx_bpf_dispatch
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. dispatch pid=%d dsq=%llu slice=%llu enq_flags=%llu\n",
        nsecs, cpu, args.p->pid, args.dsq_id, args.slice, args.enq_flags);
}

fexit:scx_bpf_dispatch_vtime
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. dispatch_vtime pid=%d dsq=%llu slice=%llu vtime=%llu enq_flags=%llu\n",
        nsecs, cpu, args.p->pid, args.dsq_id, args.slice, args.vtime, args.enq_flags);
}

fexit:scx_bpf_pick_idle_cpu
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. pick_idle_cpu flags=%llu ret=%d\n",
        nsecs, cpu, args.flags, retval);
}

fexit:scx_bpf_pick_any_cpu
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. pick_any_cpu flags=%llu ret=%d\n",
        nsecs, cpu, args.flags, retval);
}

fexit:scx_bpf_kick_cpu
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. kick_cpu target=%d flags=%llu\n",
        nsecs, cpu, args.cpu, args.flags);
}

fexit:scx_bpf_task_cpu
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. task_cpu pid=%d ret=%d\n",
        nsecs, cpu, args.p->pid, retval);
}

fexit:scx_bpf_consume
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. consume dsq=%llu ret=%d\n",
        nsecs, cpu, args.dsq_id, retval);
}

fexit:scx_bpf_create_dsq
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. create_dsq dsq=%llu node=%d ret=%d\n",
        nsecs, cpu, args.dsq_id, args.node, retval);
}

fexit:scx_bpf_dsq_nr_queued
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. dsq_nr_queued dsq=%llu ret=%d\n",
        nsecs, cpu, args.dsq_id, retval);
}

fexit:scx_bpf_task_cgroup
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. task_cgroup pid=%d\n",
        nsecs, cpu, args.p->pid);
}

fexit:scx_bpf_task_running
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. task_running pid=%d ret=%d\n",
        nsecs, cpu, args.p->pid, retval);
}

fexit:scx_bpf_reenqueue_local
/ cpu < @max_cpu / {
    printf("%llu cpu=%d .. reenqueue_local ret=%u\n",
        nsecs, cpu, retval);
}

/* ================================================================
 * LIFECYCLE TRACEPOINTS (sched_switch + sched_wakeup)
 *
 * These carry PIDs and comms natively, complementing the kprobes
 * above (which lack PIDs due to BPF complexity limits).
 * Together they give a complete picture: ops callbacks (what the
 * scheduler does) + lifecycle events (what actually happens).
 *
 * prev_state encoding for sched_switch:
 *   0 = TASK_RUNNING (preempted)
 *   1 = TASK_INTERRUPTIBLE (voluntary sleep)
 *   2 = TASK_UNINTERRUPTIBLE (uninterruptible sleep)
 * ================================================================ */

tracepoint:sched:sched_switch
/ cpu < @max_cpu / {
    printf("%llu cpu=%d == sched_switch prev_pid=%d prev_comm=%s prev_state=%ld next_pid=%d next_comm=%s\n",
        nsecs, cpu, args.prev_pid, args.prev_comm, args.prev_state, args.next_pid, args.next_comm);
}

tracepoint:sched:sched_wakeup
/ cpu < @max_cpu / {
    printf("%llu cpu=%d == sched_wakeup pid=%d comm=%s target_cpu=%d\n",
        nsecs, cpu, args.pid, args.comm, args.target_cpu);
}

END {
    clear(@max_cpu);
    printf("\nDone. Captured ops + kfunc trace.\n");
}
