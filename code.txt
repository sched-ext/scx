]633;E;for file in scheds/rust/scx_mitosis/src/bpf/*;7dc75c10-53e2-4af4-8cab-ea0159bd7502]633;C# File: scheds/rust/scx_mitosis/src/bpf/dsq.bpf.h
/* Copyright (c) Meta Platforms, Inc. and affiliates. */
/*
 * This software may be used and distributed according to the terms of the
 * GNU General Public License version 2.
 *
 * This header defines the 64-bit dispatch queue (DSQ) ID encoding
 * scheme for scx_mitosis, using type fields to distinguish between
 * per-CPU and cell+L3 domain queues. It includes helper functions to
 * construct, validate, and parse these DSQ IDs for queue management.
 */
#pragma once

#include "intf.h"
#include "mitosis.bpf.h"

/*
 * ================================
 * BPF DSQ ID Layout (64 bits wide)
 * ================================
 *
 * Top-level format:
 *   [63] [62..0]
 *   [ B] [  ID ]
 *
 * If B == 1 it is a Built-in DSQ
 * -------------------------
 *   [63] [62] [61 .. 32]  [31..0]
 *   [ 1] [ L] [   R    ]  [  V  ]
 *
 *   - L (bit 62): LOCAL_ON flag
 *       If L == 1 -> V = CPU number
 *   - R (30 bits): reserved / unused
 *   - V (32 bits): value (e.g., CPU#)
 *
 * If B == 0 -> User-defined DSQ
 * -----------------------------
 * Only the low 32 bits are used.
 *
 *   [63     ..     32] [31..0]
 *   [ 0][   unused   ] [ VAL ]
 *
 *   Mitosis uses VAL as follows:
 *
 *   [31..28] [27..0]
 *   [QTYPE ] [DATA ]
 *
 *   QTYPE encodes the queue type:
 *
 *     QTYPE = 0x1 -> Per-CPU Q
 *       [31..28] [27 ..          ..        0]
 *       [ 0001 ] [          CPU#            ]
 *       [Q-TYPE:1]
 *
 *     QTYPE = 0x2 -> Cell+L3 Q
 *       [31..28] [27 .. 16] [15      ..    0]
 *       [ 0010 ] [  CELL# ] [      L3ID     ]
 *       [Q-TYPE:2]
 *
 */
/*
 * The use of these bitfields depends on compiler defined byte AND bit ordering.
 * Make sure we're only building with Clang/LLVM and that we're little-endian.
 */
#ifndef __clang__
#error "This code must be compiled with Clang/LLVM (eBPF: clang -target bpf)."
#endif

#if __BYTE_ORDER__ != __ORDER_LITTLE_ENDIAN__
#error "dsq64 bitfield layout assumes little-endian (bpfel)."
#endif

/* ---- Bitfield widths (bits) ---- */
#define CPU_B     28
#define L3_B      16
#define CELL_B    12
#define TYPE_B     4
#define DATA_B    28
#define RSVD_B    32

/* Sum checks (in bits) */
_Static_assert(CPU_B  + TYPE_B          == 32, "CPU layout low half must be 32 bits");
_Static_assert(L3_B   + CELL_B + TYPE_B == 32, "CELL+L3 layout low half must be 32 bits");
_Static_assert(DATA_B + TYPE_B          == 32, "Common layout low half must be 32 bits");

typedef union {
	u64 raw;

	/* Per-CPU user DSQ */
	struct { u64 cpu: CPU_B;   u64 type: TYPE_B; u64 rsvd: RSVD_B; } cpu_dsq;

	/* Cell+L3 user DSQ */
	struct { u64 l3: L3_B;     u64 cell: CELL_B; u64 type: TYPE_B; u64 rsvd: RSVD_B; } cell_l3_dsq;

	/* Generic user view */
	struct { u64 data: DATA_B; u64 type: TYPE_B; u64 rsvd: RSVD_B; } user_dsq;

	/* Built-in DSQ view */
	struct { u64 value:32; u64 rsvd:30; u64 local_on:1; u64 builtin:1; } builtin_dsq;

	/* NOTE: Considered packed and aligned attributes, but that's redundant */
} dsq_id_t;

/*
 * Invalid DSQ ID Sentinel:
 * invalid bc bit 63 clear (it's a user DSQ) && dsq_type == 0 (no type)
 * Good for catching uninitialized DSQ IDs.
*/
#define DSQ_INVALID ((u64) 0)

_Static_assert(sizeof(((dsq_id_t){0}).cpu_dsq)     == sizeof(u64), "cpu view must be 8 bytes");
_Static_assert(sizeof(((dsq_id_t){0}).cell_l3_dsq) == sizeof(u64), "cell+l3 view must be 8 bytes");
_Static_assert(sizeof(((dsq_id_t){0}).user_dsq)    == sizeof(u64), "user common view must be 8 bytes");
_Static_assert(sizeof(((dsq_id_t){0}).builtin_dsq) == sizeof(u64), "builtin view must be 8 bytes");

/* Compile-time checks (in bytes) */
_Static_assert(sizeof(dsq_id_t)   == sizeof(u64), "dsq_id_t must be 8 bytes (64 bits)");
_Static_assert(_Alignof(dsq_id_t) == sizeof(u64), "dsq_id_t must be 8-byte aligned");

/* DSQ type enumeration */
enum dsq_type {
	DSQ_TYPE_NONE,
	DSQ_TYPE_CPU,
	DSQ_TYPE_CELL_L3,
};

/* Range guards */
_Static_assert(MAX_CPUS  <= (1u << CPU_B),  "MAX_CPUS must fit in field");
_Static_assert(MAX_L3S   <= (1u << L3_B),   "MAX_L3S must fit in field");
_Static_assert(MAX_CELLS <= (1u << CELL_B), "MAX_CELLS must fit in field");
_Static_assert(DSQ_TYPE_CELL_L3 < (1u << TYPE_B), "DSQ_TYPE_CELL_L3 must fit in field");

/*
 * While I considered error propagation, I decided to bail to force errors early.
*/

static inline bool is_user_dsq(dsq_id_t dsq_id){
	return !dsq_id.builtin_dsq.builtin && dsq_id.user_dsq.type != DSQ_TYPE_NONE;
}

// Is this a per CPU DSQ?
static inline bool is_cpu_dsq(dsq_id_t dsq_id)
{
	return is_user_dsq(dsq_id) && dsq_id.user_dsq.type == DSQ_TYPE_CPU;
}

// If this is a per cpu dsq, return the cpu
static inline u32 get_cpu_from_dsq(dsq_id_t dsq_id)
{
	if (!is_cpu_dsq(dsq_id))
		scx_bpf_error("trying to get cpu from non-cpu dsq\n");

	return dsq_id.cpu_dsq.cpu;
}

/* Helper functions to construct DSQ IDs */
static inline dsq_id_t get_cpu_dsq_id(u32 cpu)
{
	// Check for valid CPU range, 0 indexed so >=.
	if (cpu >= MAX_CPUS)
		scx_bpf_error("invalid cpu %u\n", cpu);

	return (dsq_id_t){ .cpu_dsq = { .cpu = cpu, .type = DSQ_TYPE_CPU } };
}

static inline dsq_id_t get_cell_l3_dsq_id(u32 cell, u32 l3)
{
	if (cell >= MAX_CELLS || l3 >= MAX_L3S)
		scx_bpf_error("cell %u or l3 %u too large\n", cell, l3);

	return (dsq_id_t){ .cell_l3_dsq = { .l3 = l3, .cell = cell, .type = DSQ_TYPE_CELL_L3 } };
}
# File: scheds/rust/scx_mitosis/src/bpf/intf.h
// Copyright (c) Meta Platforms, Inc. and affiliates.

// This software may be used and distributed according to the terms of the
// GNU General Public License version 2.
#ifndef __INTF_H
#define __INTF_H

#ifndef __KERNEL__
typedef unsigned long long u64;
typedef unsigned int u32;
typedef _Bool bool;
#endif

#ifdef LSP
#define __bpf__
#include "../../../../include/scx/ravg.bpf.h"
#else
#include <scx/ravg.bpf.h>
#endif

/* ---- Work stealing config (compile-time) ------------------------------- */
#ifndef MITOSIS_ENABLE_STEALING
#define MITOSIS_ENABLE_STEALING 1
#endif
/* ----------------------------------------------------------------------- */

enum consts {
	CACHELINE_SIZE = 64,
	MAX_CPUS_SHIFT = 9,
	MAX_CPUS = 1 << MAX_CPUS_SHIFT,
	MAX_CPUS_U8 = MAX_CPUS / 8,
	MAX_CELLS = 16,
	USAGE_HALF_LIFE = 100000000, /* 100ms */

	PCPU_BASE = 0x80000000,
	MAX_CG_DEPTH = 256,
};

/* Statistics */
enum cell_stat_idx {
	CSTAT_LOCAL,
	CSTAT_CPU_DSQ,
	CSTAT_CELL_DSQ,
	CSTAT_AFFN_VIOL,
	NR_CSTATS,
};

/* Function invocation counters */
enum fn_counter_idx {
	COUNTER_SELECT_CPU,
	COUNTER_ENQUEUE,
	COUNTER_DISPATCH,
	NR_COUNTERS,
};

struct cpu_ctx {
	u64 cstats[MAX_CELLS][NR_CSTATS];
	u64 cell_cycles[MAX_CELLS];
	u32 cell;
	u64 vtime_now;
};

struct cgrp_ctx {
	u32 cell;
	bool cell_owner;
};

#endif /* __INTF_H */
# File: scheds/rust/scx_mitosis/src/bpf/l3_aware.bpf.h
/* Copyright (c) Meta Platforms, Inc. and affiliates. */
/*
 * This software may be used and distributed according to the terms of the
 * GNU General Public License version 2.
 *
 * This header assists adding L3 cache awareness to scx_mitosis by defining
 * maps and fns for managing CPU-to-L3 domain mappings. It provides code to
 * recalculate per-L3 CPU counts within cells and implements weighted
 * random L3 selection for tasks. It also tracks work-stealing
 * statistics for cross-L3 task migrations.
 */
#pragma once

#include "mitosis.bpf.h"
#include "intf.h"

typedef u32 l3_id_t;
#define L3_INVALID ((l3_id_t)~0u)

// Configure how aggressively we steal work.
// When task is detected as a steal candidate, skip it this many times
// On a web server workload, 100 reduced steal count by ~90%
#ifdef MITOSIS_ENABLE_STEALING
#define PREVENT_N_STEALS 0
#endif

/* Work stealing statistics map - accessible from both BPF and userspace */
struct steal_stats_map {
	__uint(type, BPF_MAP_TYPE_ARRAY);
	__type(key, u32);
	__type(value, u64);
	__uint(max_entries, 1);
};

// A CPU -> L3 cache ID map
struct cpu_to_l3_map {
	__uint(type, BPF_MAP_TYPE_ARRAY);
	__type(key, u32);
	__type(value, u32);
	__uint(max_entries, MAX_CPUS);
};

struct l3_to_cpus_map {
	__uint(type, BPF_MAP_TYPE_ARRAY);
	__type(key, u32);
	__type(value, struct cpumask);
	__uint(max_entries, MAX_L3S);
};

extern struct cpu_to_l3_map cpu_to_l3;
extern struct l3_to_cpus_map l3_to_cpus;
extern struct steal_stats_map steal_stats;

static inline const bool l3_is_valid(u32 l3_id)
{
	if (l3_id == L3_INVALID)
		return false;

	return (l3_id >= 0) && (l3_id < MAX_L3S);
}

static inline void init_task_l3(struct task_ctx *tctx)
{
	tctx->l3 = L3_INVALID;

#if MITOSIS_ENABLE_STEALING
	tctx->pending_l3 = L3_INVALID;
	tctx->steal_count = 0;
	tctx->last_stolen_at = 0;
	tctx->steals_prevented = 0;
#endif
}

static inline const struct cpumask *lookup_l3_cpumask(u32 l3)
{
	struct cpumask *mask;

	if (!(mask = bpf_map_lookup_elem(&l3_to_cpus, &l3))) {
		scx_bpf_error("no l3 cpumask, l3: %d, %p", l3, &l3_to_cpus);
		return NULL;
	}

	return mask;
}

/* Recompute cell->l3_cpu_cnt[] after cell cpumask changes */
// TODO: use RAII and lock around updates (races with )
static __always_inline void recalc_cell_l3_counts(u32 cell_idx)
{
	struct cell *cell = lookup_cell(cell_idx);
	if (!cell) {
		scx_bpf_error("recalc_cell_l3_counts: invalid cell %d",
			      cell_idx);
		return;
	}

	CPUMASK_GUARD(tmp_guard);
	if (!tmp_guard.mask) {
		scx_bpf_error(
			"recalc_cell_l3_counts: failed to create tmp mask");
		return;
	}

	u32 l3, l3s_present = 0, total_cpus = 0;
	// Just so we don't hold the lock longer than necessary
	u32 l3_cpu_cnt_tmp[MAX_L3S] = {0};

	{ // RCU context
		RCU_READ_GUARD();
		const struct cpumask *cell_mask =
			lookup_cell_cpumask(cell_idx); // RCU ptr

		if (!cell_mask) {
			scx_bpf_error(
				"recalc_cell_l3_counts: invalid cell mask");
			return;
		}

		bpf_for(l3, 0, nr_l3)
		{
			const struct cpumask *l3_mask = lookup_l3_cpumask(l3);
			if (!l3_mask) {
				scx_bpf_error(
					"recalc_cell_l3_counts: invalid l3 mask");
				return;
			}

			bpf_cpumask_and(tmp_guard.mask, cell_mask, l3_mask);

			u32 cnt = bpf_cpumask_weight( (const struct cpumask *)tmp_guard.mask);

			l3_cpu_cnt_tmp[l3] = cnt;

			bpf_printk("recalc_cell_l3_counts: cnt %d", cnt);

			// These are counted across the whole cell
			total_cpus += cnt;

			if (cnt)
				l3s_present++;
		}
	} // bpf_rcu_read_unlock();

	// WITH_CELL_LOCK(cell, cell_idx, {
	for (u32 l3 = 0; l3 < nr_l3; l3++) {
		cell->l3_cpu_cnt[l3] = l3_cpu_cnt_tmp[l3];
	}

	cell->l3_present_cnt = l3s_present;
	cell->cpu_cnt = total_cpus;
	// });
}

/**
 * Weighted random selection of an L3 cache domain for a task.
 *
 * Uses the CPU count in each L3 domain within the cell as weights to
 * probabilistically select an L3. L3 domains with more CPUs in the cell
 * have higher probability of being selected.
 *
 * @cell_id: The cell ID to select an L3 from
 * @return: L3 ID on success, L3_INVALID on error
 */
// TODO: Lock
static inline s32 pick_l3_for_task(u32 cell_id)
{
	struct cell *cell;

	/* Look up the cell structure */
	if (!(cell = lookup_cell(cell_id))) {
		scx_bpf_error("pick_l3_for_task: invalid cell %d", cell_id);
		return L3_INVALID;
	}

	// No cells
	if (!cell->cpu_cnt) {
		scx_bpf_error( "pick_l3_for_task: cell %d has no CPUs accounted yet", cell_id);
		return L3_INVALID;
	}

	/* Find the L3 domain corresponding to the target value using
	 * weighted selection - accumulate CPU counts until we exceed target */

	/* Generate random target value in range [0, cpu_cnt) */
	u32 target = bpf_get_prandom_u32() % cell->cpu_cnt;
	u32 l3, cur = 0;
	s32 ret = L3_INVALID;

	// This could be a prefix sum. Find first l3 where we exceed target
	bpf_for(l3, 0, nr_l3)
	{
		cur += cell->l3_cpu_cnt[l3];
		if (target < cur) {
			ret = (s32)l3;
			break;
		}
	}

	if (ret == L3_INVALID) {
		scx_bpf_error("pick_l3_for_task: invalid L3");
		return L3_INVALID;
	}

	return ret;
}

#ifdef MITOSIS_ENABLE_STEALING

static inline bool try_stealing_this_task(struct task_ctx *task_ctx,
					  s32 local_l3, u64 candidate_dsq)
{
	// Attempt the steal, can fail beacuse it's a race.
	if (!scx_bpf_dsq_move_to_local(candidate_dsq))
		return false;

	// We got the task!
	task_ctx->steal_count++;
	task_ctx->last_stolen_at = scx_bpf_now();
	/* Retag to thief L3 (the one for this cpu) */
	task_ctx->pending_l3 = local_l3;
	task_ctx->steals_prevented = 0;

	/* Increment steal counter in map */
	u32 key = 0;
	u64 *count = bpf_map_lookup_elem(&steal_stats, &key);
	// NOTE: This could get expensive, but I'm not anticipating that many steals. Percpu if we care.
	if (count)
		__sync_fetch_and_add(count, 1);

	return true;
}

/* Work stealing:
 * Scan sibling (cell,L3) DSQs in the same cell and steal the first queued task if it can run on this cpu
*/
static inline bool try_stealing_work(u32 cell, s32 local_l3)
{
	if (!l3_is_valid(local_l3))
		scx_bpf_error("try_stealing_work: invalid local_l3");

	struct cell *cell_ptr = lookup_cell(cell);
	if (!cell_ptr)
		scx_bpf_error("try_stealing_work: invalid cell");

	// Loop over all other L3s, looking for a queued task to steal
	u32 i;
	bpf_for(i, 1, nr_l3)
	{
		// Start with the next one to spread out the load
		u32 candidate_l3 = (local_l3 + i) % nr_l3;

		// Prevents the optimizer from removing the following conditional return
		// so that the verifier knows the read wil be safe
		barrier_var(candidate_l3);

		if (candidate_l3 >= MAX_L3S)
			continue;

		// Skip L3s that are not present in this cell
		// Note: rechecking cell_ptr for verifier
		// TODO: Lock?
		if (cell_ptr && cell_ptr->l3_cpu_cnt[candidate_l3] == 0)
			continue;

		u64 candidate_dsq = get_cell_l3_dsq_id(cell, candidate_l3).raw;

		struct task_struct *task = NULL;
		struct task_ctx *task_ctx;
		// I'm only using this for the verifier
		bool found_task = false;

		// Optimization: skip if faster than constructing an iterator
		// Not redundant with later checking if task found (race)
		if (scx_bpf_dsq_nr_queued(candidate_dsq))
			continue;

		// Just a trick for peeking the head element
		bpf_for_each(scx_dsq, task, candidate_dsq, 0)
		{
			task_ctx = lookup_task_ctx(task);
			found_task = (task_ctx != NULL);
			break;
		}

		// No task? Try next L3
		if (!found_task)
			continue;

		// This knob throttles stealing.
		// TODO: make runtime configurable
		if (task_ctx->steals_prevented++ < PREVENT_N_STEALS) {
			continue;
		}

		if (!try_stealing_this_task(task_ctx, local_l3, candidate_dsq))
			continue;

		// Success, we got a task (no guarantee it was the one we peeked though... race)
		return true;
	}
	return false;
}
#endif
# File: scheds/rust/scx_mitosis/src/bpf/mitosis.bpf.c
/* Copyright (c) Meta Platforms, Inc. and affiliates. */
/*
 * This software may be used and distributed according to the terms of the
 * GNU General Public License version 2.
 *
 * scx_mitosis is a dynamic affinity scheduler. Cgroups (and their tasks) are
 * assigned to Cells which are affinitized to discrete sets of CPUs. The number
 * of cells is dynamic, as is cgroup to cell assignment and cell to CPU
 * assignment (all are determined by userspace).
 *
 * Each cell has an associated DSQ which it uses for vtime scheduling of the
 * cgroups belonging to the cell.
 */

// TODO: fix debug printer.
#include "intf.h"

#include "mitosis.bpf.h"
#include "dsq.bpf.h"
#include "l3_aware.bpf.h"

char _license[] SEC("license") = "GPL";

/*
 * Variables populated by userspace
 */
const volatile u32 nr_possible_cpus = 1;
const volatile bool smt_enabled = true;
const volatile unsigned char all_cpus[MAX_CPUS_U8];

const volatile u64 slice_ns;
const volatile u64 root_cgid = 1;

const volatile u32 nr_l3 = 1;
/*
 * CPU assignment changes aren't fully in effect until a subsequent tick()
 * configuration_seq is bumped on each assignment change
 * applied_configuration_seq is bumped when the effect is fully applied
 */
u32 configuration_seq;
u32 applied_configuration_seq;

private(all_cpumask) struct bpf_cpumask __kptr *all_cpumask;
private(root_cgrp) struct cgroup __kptr *root_cgrp;

UEI_DEFINE(uei);

// Cells now defined as a map so we can lock.
struct cell_map cells SEC(".maps");

/*
 * Maps used for L3-aware scheduling
*/
#if 0
struct cell_locks_map cell_locks SEC(".maps");
#endif
struct cpu_to_l3_map cpu_to_l3 SEC(".maps");
struct l3_to_cpus_map l3_to_cpus SEC(".maps");

/*
 * Maps for statistics
*/
struct function_counters_map function_counters SEC(".maps");
struct steal_stats_map steal_stats SEC(".maps");

static inline void increment_counter(enum fn_counter_idx idx) {
	u64 *counter;
	u32 key = idx;

	counter = bpf_map_lookup_elem(&function_counters, &key);
	if (counter)
		(*counter)++;
}

static inline struct cgroup *lookup_cgrp_ancestor(struct cgroup *cgrp,
						  u32 ancestor)
{
	struct cgroup *cg;

	if (!(cg = bpf_cgroup_ancestor(cgrp, ancestor))) {
		scx_bpf_error("Failed to get ancestor level %d for cgid %llu",
			      ancestor, cgrp->kn->id);
		return NULL;
	}

	return cg;
}

struct {
	__uint(type, BPF_MAP_TYPE_CGRP_STORAGE);
	__uint(map_flags, BPF_F_NO_PREALLOC);
	__type(key, int);
	__type(value, struct cgrp_ctx);
} cgrp_ctxs SEC(".maps");

static inline struct cgrp_ctx *lookup_cgrp_ctx_fallible(struct cgroup *cgrp)
{
	struct cgrp_ctx *cgc;

	if (!(cgc = bpf_cgrp_storage_get(&cgrp_ctxs, cgrp, 0, 0))) {
		return NULL;
	}

	return cgc;
}

static inline struct cgrp_ctx *lookup_cgrp_ctx(struct cgroup *cgrp)
{
	struct cgrp_ctx *cgc = lookup_cgrp_ctx_fallible(cgrp);

	if (!cgc)
		scx_bpf_error("cgrp_ctx lookup failed for cgid %llu",
			      cgrp->kn->id);

	return cgc;
}

static inline struct cgroup *task_cgroup(struct task_struct *p)
{
	struct cgroup *cgrp = __COMPAT_scx_bpf_task_cgroup(p);
	if (!cgrp) {
		scx_bpf_error("Failed to get cgroup for task %d", p->pid);
	}
	return cgrp;
}

struct {
	__uint(type, BPF_MAP_TYPE_TASK_STORAGE);
	__uint(map_flags, BPF_F_NO_PREALLOC);
	__type(key, int);
	__type(value, struct task_ctx);
} task_ctxs SEC(".maps");

static inline struct task_ctx *lookup_task_ctx(struct task_struct *p)
{
	struct task_ctx *tctx;

	if ((tctx = bpf_task_storage_get(&task_ctxs, p, 0, 0))) {
		return tctx;
	}

	scx_bpf_error("task_ctx lookup failed");
	return NULL;
}

struct {
	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
	__type(key, u32);
	__type(value, struct cpu_ctx);
	__uint(max_entries, 1);
} cpu_ctxs SEC(".maps");

static inline struct cpu_ctx *lookup_cpu_ctx(int cpu)
{
	struct cpu_ctx *cctx;
	u32 zero = 0;

	if (cpu < 0)
		cctx = bpf_map_lookup_elem(&cpu_ctxs, &zero);
	else
		cctx = bpf_map_lookup_percpu_elem(&cpu_ctxs, &zero, cpu);

	if (!cctx) {
		scx_bpf_error("no cpu_ctx for cpu %d", cpu);
		return NULL;
	}

	return cctx;
}



/*
 * Cells are allocated concurrently in some cases (e.g. cgroup_init).
 * allocate_cell and free_cell enable these allocations to be done safely
 */
static inline int allocate_cell()
{
	int cell_idx;
	bpf_for(cell_idx, 0, MAX_CELLS)
	{
		struct cell *c;
		if (!(c = lookup_cell(cell_idx)))
			return -1;

		if (__sync_bool_compare_and_swap(&c->in_use, 0, 1)) {
			// TODO XXX, I think we need to make this concurrent safe
			// TODO, lock with recalc_cell...()
			__builtin_memset(c->l3_cpu_cnt, 0, sizeof(c->l3_cpu_cnt));
			c->l3_present_cnt = 0;
			// TODO zero cpu_cnt
			// TODO Just zero the whole cell struct?
			return cell_idx;
		}
	}
	scx_bpf_error("No available cells to allocate");
	return -1;
}

static inline int free_cell(int cell_idx)
{
	struct cell *c;

	if (cell_idx < 0 || cell_idx >= MAX_CELLS) {
		scx_bpf_error("Invalid cell %d", cell_idx);
		return -1;
	}

	if (!(c = lookup_cell(cell_idx)))
		return -1;

	WRITE_ONCE(c->in_use, 0);
	return 0;
}

/*
 * Store the cpumask for each cell (owned by BPF logic). We need this in an
 * explicit map to allow for these to be kptrs.
 */
struct cell_cpumask_wrapper {
	struct bpf_cpumask __kptr *cpumask;
	/*
	 * To avoid allocation on the reconfiguration path, have a second cpumask we
	 * can just do an xchg on.
	 */
	struct bpf_cpumask __kptr *tmp_cpumask;
};

struct {
	__uint(type, BPF_MAP_TYPE_ARRAY);
	__type(key, u32);
	__type(value, struct cell_cpumask_wrapper);
	__uint(max_entries, MAX_CELLS);
	__uint(map_flags, 0);
} cell_cpumasks SEC(".maps");

static inline const struct cpumask *lookup_cell_cpumask(int idx)
{
	struct cell_cpumask_wrapper *cpumaskw;

	if (!(cpumaskw = bpf_map_lookup_elem(&cell_cpumasks, &idx))) {
		scx_bpf_error("no cell cpumask");
		return NULL;
	}

	return (const struct cpumask *)cpumaskw->cpumask;
}

/*
 * Helper functions for bumping per-cell stats
 */
static void cstat_add(enum cell_stat_idx idx, u32 cell, struct cpu_ctx *cctx,
		      s64 delta)
{
	u64 *vptr;

	if ((vptr = MEMBER_VPTR(*cctx, .cstats[cell][idx])))
		(*vptr) += delta;
	else
		scx_bpf_error("invalid cell or stat idxs: %d, %d", idx, cell);
}

static void cstat_inc(enum cell_stat_idx idx, u32 cell, struct cpu_ctx *cctx)
{
	cstat_add(idx, cell, cctx, 1);
}

static inline int update_task_cpumask(struct task_struct *p,
				      struct task_ctx *tctx)
{
	const struct cpumask *cell_cpumask;
	struct cpu_ctx *cpu_ctx;
	u32 cpu;

	if (!(cell_cpumask = lookup_cell_cpumask(tctx->cell)))
		return -ENOENT;

	if (!tctx->cpumask)
		return -EINVAL;

	/*
	 * Calculate the intersection of CPUs that are both:
	 * 1. In this task's assigned cell (cell_cpumask)
	 * 2. Allowed by the task's CPU affinity (p->cpus_ptr)
	 * Store result in tctx->cpumask - this becomes the effective CPU set
	 * where this task can actually run.
	 */
	bpf_cpumask_and(tctx->cpumask, cell_cpumask, p->cpus_ptr);

	/*
	 * Check if the task can run on ALL CPUs in its assigned cell.
	 * If cell_cpumask is a subset of p->cpus_ptr, it means the task's
	 * CPU affinity doesn't restrict it within the cell - it can use
	 * any CPU in the cell. This affects scheduling decisions later.
	 * True if all the bits in cell_cpumask are set in p->cpus_ptr.
	 */
	tctx->all_cell_cpus_allowed =
		bpf_cpumask_subset(cell_cpumask, p->cpus_ptr);

	/*
	 * XXX - To be correct, we'd need to calculate the vtime
	 * delta in the previous dsq, scale it by the load
	 * fraction difference and then offset from the new
	 * dsq's vtime_now. For now, just do the simple thing
	 * and assume the offset to be zero.
	 *
	 * Revisit if high frequency dynamic cell switching
	 * needs to be supported.
	 */

	// We want to set the task vtime to that of the cell it's joining.
	if (tctx->all_cell_cpus_allowed) {

		const struct cpumask *l3_mask = NULL;
		if (tctx->l3 != L3_INVALID) {
			l3_mask = lookup_l3_cpumask((u32)tctx->l3);
			/* If the L3 no longer intersects the cell's cpumask, invalidate it */
			if (!l3_mask || !bpf_cpumask_intersects(cell_cpumask, l3_mask))
				tctx->l3 = L3_INVALID;
		}

		/* --- Pick a new L3 if needed --- */
		if (tctx->l3 == L3_INVALID) {
			s32 new_l3 = pick_l3_for_task(tctx->cell);
			if (new_l3 < 0) {
				scx_bpf_error("bad L3: %d", new_l3);
				return -ENODEV;
			}
			tctx->l3 = new_l3;
			l3_mask = lookup_l3_cpumask((u32)tctx->l3);
			if (!l3_mask)
				return -ENOENT;
		}

		/* --- Narrow the effective cpumask by the chosen L3 --- */
		/* tctx->cpumask already contains (task_affinity âˆ§ cell_mask) */
		if (tctx->cpumask)
			bpf_cpumask_and(tctx->cpumask, (const struct cpumask *)tctx->cpumask, l3_mask);

		/* If empty after intersection, nothing can run here */
		if (tctx->cpumask && bpf_cpumask_empty((const struct cpumask *)tctx->cpumask)) {
			scx_bpf_error("Empty cpumask after intersection");
			return -ENODEV;
		}

		/* --- Point to the correct (cell,L3) DSQ and set vtime baseline --- */
		tctx->dsq = get_cell_l3_dsq_id(tctx->cell, tctx->l3);

		struct cell *cell = lookup_cell(tctx->cell);
		if (!cell)
			return -ENOENT;

		if (!l3_is_valid(tctx->l3)){
			scx_bpf_error("Invalid L3 %d", tctx->l3);
			return -EINVAL;
		}

		p->scx.dsq_vtime = READ_ONCE(cell->l3_vtime_now[tctx->l3]);
	} else {
		/* Task is CPU-restricted, use task mask */
		cpu = bpf_cpumask_any_distribute(p->cpus_ptr);
		if (!(cpu_ctx = lookup_cpu_ctx(cpu)))
			return -ENOENT;
		tctx->dsq = get_cpu_dsq_id(cpu);
		p->scx.dsq_vtime = READ_ONCE(cpu_ctx->vtime_now);
	}

	return 0;
}

/*
 * Figure out the task's cell, dsq and store the corresponding cpumask in the
 * task_ctx.
 */
static inline int update_task_cell(struct task_struct *p, struct task_ctx *tctx,
				   struct cgroup *cg)
{
	struct cgrp_ctx *cgc;

	if (!(cgc = lookup_cgrp_ctx(cg)))
		return -ENOENT;

	/*
	 * This ordering is pretty important, we read applied_configuration_seq
	 * before reading everything else expecting that the updater will update
	 * everything and then bump applied_configuration_seq last. This ensures
	 * that we cannot miss an update.
	 */
	tctx->configuration_seq = READ_ONCE(applied_configuration_seq);
	barrier();
	tctx->cell = cgc->cell;

	return update_task_cpumask(p, tctx);
}

/* Helper function for picking an idle cpu out of a candidate set */
static s32 pick_idle_cpu_from(struct task_struct *p,
			      const struct cpumask *cand_cpumask, s32 prev_cpu,
			      const struct cpumask *idle_smtmask)
{
	bool prev_in_cand = bpf_cpumask_test_cpu(prev_cpu, cand_cpumask);
	s32 cpu;

	/*
	 * If CPU has SMT, any wholly idle CPU is likely a better pick than
	 * partially idle @prev_cpu.
	 */
	if (smt_enabled) {
		if (prev_in_cand &&
		    bpf_cpumask_test_cpu(prev_cpu, idle_smtmask) &&
		    scx_bpf_test_and_clear_cpu_idle(prev_cpu))
			return prev_cpu;

		cpu = scx_bpf_pick_idle_cpu(cand_cpumask, SCX_PICK_IDLE_CORE);
		if (cpu >= 0)
			return cpu;
	}

	if (prev_in_cand && scx_bpf_test_and_clear_cpu_idle(prev_cpu))
		return prev_cpu;

	return scx_bpf_pick_idle_cpu(cand_cpumask, 0);
}

/* Check if we need to update the cell/cpumask mapping */
static __always_inline int maybe_refresh_cell(struct task_struct *p,
					      struct task_ctx *tctx)
{
	struct cgroup *cgrp;
	int ret = 0;
	if (tctx->configuration_seq != READ_ONCE(applied_configuration_seq)) {
		if (!(cgrp = task_cgroup(p)))
			return -1;
		if (update_task_cell(p, tctx, cgrp))
			ret = -1;
		bpf_cgroup_release(cgrp);
	}
	return ret;
}

static __always_inline s32 pick_idle_cpu(struct task_struct *p, s32 prev_cpu,
					 struct cpu_ctx *cctx,
					 struct task_ctx *tctx)
{
	struct cpumask *task_cpumask;
	const struct cpumask *idle_smtmask;
	s32 cpu;

	if (!(task_cpumask = (struct cpumask *)tctx->cpumask) ||
	    !(idle_smtmask = scx_bpf_get_idle_smtmask())) {
		scx_bpf_error("Failed to get task cpumask or idle smtmask");
		return -1;
	}

	/* No overlap between cell cpus and task cpus, just find some idle cpu */
	if (bpf_cpumask_empty(task_cpumask)) {
		cstat_inc(CSTAT_AFFN_VIOL, tctx->cell, cctx);
		cpu = pick_idle_cpu_from(p, p->cpus_ptr, prev_cpu,
					 idle_smtmask);
		goto out;
	}

	cpu = pick_idle_cpu_from(p, task_cpumask, prev_cpu, idle_smtmask);
out:
	scx_bpf_put_idle_cpumask(idle_smtmask);
	return cpu;
}

/*
 * select_cpu is where we update each task's cell assignment and then try to
 * dispatch to an idle core in the cell if possible
 */
s32 BPF_STRUCT_OPS(mitosis_select_cpu, struct task_struct *p, s32 prev_cpu,
		   u64 wake_flags)
{
	s32 cpu;
	struct cpu_ctx *cctx;
	struct task_ctx *tctx;

	increment_counter(COUNTER_SELECT_CPU);

	if (!(cctx = lookup_cpu_ctx(-1)) || !(tctx = lookup_task_ctx(p)))
		return prev_cpu;

	if (maybe_refresh_cell(p, tctx) < 0)
		return prev_cpu;

	/* Pinned path: only if our task really requires a per-CPU queue. */
	if (!tctx->all_cell_cpus_allowed) {
		cstat_inc(CSTAT_AFFN_VIOL, tctx->cell, cctx);
		cpu = get_cpu_from_dsq(tctx->dsq);
		if (scx_bpf_test_and_clear_cpu_idle(cpu))
			scx_bpf_dsq_insert(p, SCX_DSQ_LOCAL, slice_ns, 0);
		return cpu;
	}

	// Grab an idle core
	if ((cpu = pick_idle_cpu(p, prev_cpu, cctx, tctx)) >= 0) {
		cstat_inc(CSTAT_LOCAL, tctx->cell, cctx);
		scx_bpf_dsq_insert(p, SCX_DSQ_LOCAL, slice_ns, 0);
		return cpu;
	}

	if (!tctx->cpumask) {
		scx_bpf_error("tctx->cpumask should never be NULL");
		return prev_cpu;
	}
	/*
	 * All else failed, send it to the prev cpu (if that's valid), otherwise any
	 * valid cpu.
	 */
	if (!bpf_cpumask_test_cpu(prev_cpu, cast_mask(tctx->cpumask)) &&
	    tctx->cpumask)
		cpu = bpf_cpumask_any_distribute(cast_mask(tctx->cpumask));
	else
		cpu = prev_cpu;

	return cpu;
}

void BPF_STRUCT_OPS(mitosis_enqueue, struct task_struct *p, u64 enq_flags)
{
	struct cpu_ctx *cctx;
	struct task_ctx *tctx;
	struct cell *cell;
	s32 task_cpu = scx_bpf_task_cpu(p);
	u64 vtime = p->scx.dsq_vtime;
	s32 cpu = -1;
	u64 basis_vtime;

	increment_counter(COUNTER_ENQUEUE);

	if (!(tctx = lookup_task_ctx(p)) || !(cctx = lookup_cpu_ctx(-1)))
		return;

	if (maybe_refresh_cell(p, tctx) < 0)
		return;

	// Cpu pinned work
	if (!tctx->all_cell_cpus_allowed) {
		cpu = get_cpu_from_dsq(tctx->dsq);
	} else if (!__COMPAT_is_enq_cpu_selected(enq_flags)) {
		/*
		 * If we haven't selected a cpu, then we haven't looked for and kicked an
		 * idle CPU. Let's do the lookup now and kick at the end.
		 */
		if (!(cctx = lookup_cpu_ctx(-1)))
			return;
		cpu = pick_idle_cpu(p, task_cpu, cctx, tctx);
		if (cpu == -1)
			return;
		if (cpu == -EBUSY) {
			/*
			 * Verifier gets unhappy claiming two different pointer types for
			 * the same instruction here. This fixes it
			 */
			barrier_var(tctx);
			if (tctx->cpumask)
				cpu = bpf_cpumask_any_distribute(
					(const struct cpumask *)tctx->cpumask);
		}
	}

	if (tctx->all_cell_cpus_allowed) {
		// This is a task that can run on any cpu in the cell

		cstat_inc(CSTAT_CELL_DSQ, tctx->cell, cctx);

		/* Task can use any CPU in its cell, set basis_vtime from per-(cell, L3) vtime */
		if (!(cell = lookup_cell(tctx->cell)))
			return;

		if (!l3_is_valid(tctx->l3)) {
			scx_bpf_error("Invalid L3 ID for task %d in enqueue", p->pid);
			return;
		}
		basis_vtime = READ_ONCE(cell->l3_vtime_now[tctx->l3]);

	} else {
		// This is a task that can only run on a specific cpu
		cstat_inc(CSTAT_CPU_DSQ, tctx->cell, cctx);

		/*
		 * cctx is the local core cpu (where enqueue is running), not the core
		 * the task belongs to. Fetch the right cctx
		 */
		if (!(cctx = lookup_cpu_ctx(cpu)))
			return;
		/* Task is pinned to specific CPUs, use per-CPU DSQ */
		basis_vtime = READ_ONCE(cctx->vtime_now);
	}

	tctx->basis_vtime = basis_vtime;

	if (time_after(vtime,
		       basis_vtime + VTIME_MAX_FUTURE_MULTIPLIER * slice_ns)) {
		scx_bpf_error("vtime is too far in the future for %d", p->pid);
		return;
	}
	/*
	 * Limit the amount of budget that an idling task can accumulate
	 * to one slice.
	 */
	// TODO: Should this be time_before64?
	if (time_before(vtime, basis_vtime - slice_ns))
		vtime = basis_vtime - slice_ns;

	scx_bpf_dsq_insert_vtime(p, tctx->dsq.raw, slice_ns, vtime, enq_flags);

	/* Kick the CPU if needed */
	if (!__COMPAT_is_enq_cpu_selected(enq_flags) && cpu >= 0)
		scx_bpf_kick_cpu(cpu, SCX_KICK_IDLE);
}

void BPF_STRUCT_OPS(mitosis_dispatch, s32 cpu, struct task_struct *prev)
{
	struct cpu_ctx *cctx;
	u32 cell;

	increment_counter(COUNTER_DISPATCH);

	if (!(cctx = lookup_cpu_ctx(-1)))
		return;

	cell = READ_ONCE(cctx->cell);

	/* Start from a valid DSQ */
	dsq_id_t local_dsq = get_cpu_dsq_id(cpu);

	bool found = false;
	dsq_id_t min_vtime_dsq = local_dsq;
	u64 min_vtime = ~0ULL; /* U64_MAX */
	struct task_struct *p;

	// Get L3
	u32 cpu_key = (u32)cpu;
	u32 *l3_ptr = bpf_map_lookup_elem(&cpu_to_l3, &cpu_key);
	s32 l3 = l3_ptr ? (s32)*l3_ptr : L3_INVALID;

	/* Check the L3 queue */
	if (l3 != L3_INVALID) {
		dsq_id_t cell_l3_dsq = get_cell_l3_dsq_id(cell, l3);
		bpf_for_each(scx_dsq, p, cell_l3_dsq.raw, 0) {
			min_vtime = p->scx.dsq_vtime;
			min_vtime_dsq = cell_l3_dsq;
			found = true;
			break;
		}
	}

	/* Check the CPU DSQ for a lower vtime */
	bpf_for_each(scx_dsq, p, local_dsq.raw, 0) {
		if (!found || time_before(p->scx.dsq_vtime, min_vtime)) {
			min_vtime = p->scx.dsq_vtime;
			min_vtime_dsq = local_dsq;
			found = true;
		}
		break;
	}

	/*
	* The move_to_local can fail if we raced with some other cpu in the cell
	* and now the cell is empty. We have to ensure to try the cpu_dsq or else
	* we might never wakeup.
	*/


	if (found) {
		// We found a task in the local or cell-L3 DSQ

		// If it was in the per cpu DSQ, there is no competation, grab it and return
		if (min_vtime_dsq.raw == local_dsq.raw) {
			scx_bpf_dsq_move_to_local(min_vtime_dsq.raw);
			return;
		}

		// If it was in the cell L3 DSQ, we are competing with other cpus in the cell-l3
		// try to move it to the local DSQ
		if (scx_bpf_dsq_move_to_local(min_vtime_dsq.raw)) {
			// We won the race and got the task, return
			return;
		}
	}

#if MITOSIS_ENABLE_STEALING
	// We didn't find a task in either DSQ, or lost the race.
	// Instead of going straight to idle, attempt to steal a task from another
	// L3 in the cell.

	// Try stealing. If successful, this moves the task to the local runqueue
	try_stealing_work(cell, l3);
#endif
}

struct cpumask_entry {
	unsigned long cpumask[CPUMASK_LONG_ENTRIES];
	u64 used;
};

struct {
	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
	__type(key, u32);
	__type(value, struct cpumask_entry);
	__uint(max_entries, MAX_CPUMASK_ENTRIES);
} cgrp_init_percpu_cpumask SEC(".maps");

static inline struct cpumask_entry *allocate_cpumask_entry()
{
	int cpumask_idx;
	bpf_for(cpumask_idx, 0, MAX_CPUMASK_ENTRIES)
	{
		struct cpumask_entry *ent = bpf_map_lookup_elem(
			&cgrp_init_percpu_cpumask, &cpumask_idx);
		if (!ent) {
			scx_bpf_error("Failed to fetch cpumask_entry");
			return NULL;
		}
		if (__sync_bool_compare_and_swap(&ent->used, 0, 1))
			return ent;
	}
	scx_bpf_error("All cpumask entries are in use");
	return NULL;
}

static inline void free_cpumask_entry(struct cpumask_entry *entry)
{
	WRITE_ONCE(entry->used, 0);
}

/* For use by cleanup attribute */
static inline void __free_cpumask_entry(struct cpumask_entry **entry)
{
	if (entry)
		if (*entry)
			free_cpumask_entry(*entry);
}

#define DECLARE_CPUMASK_ENTRY(var) \
	struct cpumask_entry *var __attribute__((cleanup(__free_cpumask_entry)))

/* Define types for cpumasks in-situ vs as a ptr in struct cpuset */
struct cpumask___local {};

typedef struct cpumask___local *cpumask_var_t___ptr;

struct cpuset___cpumask_ptr {
	cpumask_var_t___ptr cpus_allowed;
};

typedef struct cpumask___local cpumask_var_t___arr[1];

struct cpuset___cpumask_arr {
	cpumask_var_t___arr cpus_allowed;
};

/*
 * Given a cgroup, get its cpumask (populated in entry), returns 0 if no
 * cpumask, < 0 on error and > 0 on a populated cpumask.
 */
static inline int get_cgroup_cpumask(struct cgroup *cgrp,
				     struct cpumask_entry *entry)
{
	if (!cgrp->subsys[cpuset_cgrp_id])
		return 0;

	struct cpuset *cpuset =
		container_of(cgrp->subsys[cpuset_cgrp_id], struct cpuset, css);

	if (!cpuset)
		return 0;

	unsigned long runtime_cpumask_size = bpf_core_type_size(struct cpumask);
	if (runtime_cpumask_size > CPUMASK_SIZE) {
		scx_bpf_error(
			"Definition of struct cpumask is too large. Please increase CPUMASK_LONG_ENTRIES");
		return -EINVAL;
	}

	int err;
	if (bpf_core_type_matches(struct cpuset___cpumask_arr)) {
		struct cpuset___cpumask_arr *cpuset_typed =
			(void *)bpf_core_cast(cpuset, struct cpuset);
		err = bpf_core_read(&entry->cpumask, runtime_cpumask_size,
				    &cpuset_typed->cpus_allowed);
	} else if (bpf_core_type_matches(struct cpuset___cpumask_ptr)) {
		struct cpuset___cpumask_ptr *cpuset_typed =
			(void *)bpf_core_cast(cpuset, struct cpuset);
		err = bpf_core_read(&entry->cpumask, runtime_cpumask_size,
				    cpuset_typed->cpus_allowed);
	} else {
		scx_bpf_error(
			"Definition of struct cpuset did not match any expected struct");
		return -EINVAL;
	}

	if (err < 0) {
		scx_bpf_error(
			"bpf_core_read of cpuset->cpus_allowed failed for cgid %llu",
			cgrp->kn->id);
		return err;
	}

	if (bpf_cpumask_empty((const struct cpumask *)&entry->cpumask))
		return 0;

	if (!all_cpumask) {
		scx_bpf_error("all_cpumask should not be NULL");
		return -EINVAL;
	}

	if (bpf_cpumask_subset((const struct cpumask *)all_cpumask,
			       (const struct cpumask *)&entry->cpumask))
		return 0;

	return 1;
}

/*
 * This array keeps track of the cgroup ancestor's cell as we iterate over the
 * cgroup hierarchy.
 */
u32 level_cells[MAX_CG_DEPTH];
int running;

/* The guard is a stack variable. When it falls out of scope,
 * we drop the running lock. */
static inline void __running_unlock(int *guard) {
	(void)guard; /* unused */
	WRITE_ONCE(running, 0);
}

/*
 * On tick, we identify new cells and apply CPU assignment
 */
void BPF_STRUCT_OPS(mitosis_tick, struct task_struct *p_run)
{

	u32 local_configuration_seq = READ_ONCE(configuration_seq);
	if (local_configuration_seq == READ_ONCE(applied_configuration_seq))
		return;

	int zero = 0;
	if (!__atomic_compare_exchange_n(&running, &zero, 1, false,
					 __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST))
		return;

	int __attribute__((cleanup(__running_unlock), unused)) __running_guard;

	DECLARE_CPUMASK_ENTRY(entry) = allocate_cpumask_entry();
	if (!entry)
		return;

	/* Get the root cell (cell 0) and its cpumask */
	struct cell_cpumask_wrapper *root_cell_cpumaskw;
	if (!(root_cell_cpumaskw =
		      bpf_map_lookup_elem(&cell_cpumasks, &zero))) {
		scx_bpf_error("Failed to find root cell cpumask");
		return;
	}

	struct bpf_cpumask *root_bpf_cpumask;
	root_bpf_cpumask =
		bpf_kptr_xchg(&root_cell_cpumaskw->tmp_cpumask, NULL);
	if (!root_bpf_cpumask) {
		scx_bpf_error("tmp_cpumask should never be null");
		return;
	}
	if (!root_cell_cpumaskw->cpumask) {
		scx_bpf_error("root cpumasks should never be null");
		goto out;
	}

	if (!all_cpumask) {
		scx_bpf_error("NULL all_cpumask");
		goto out;
	}

	/*
	 * Initialize root cell cpumask to all cpus, and then remove from it as we go
	 */
	bpf_cpumask_copy(root_bpf_cpumask, (const struct cpumask *)all_cpumask);

	struct cgroup_subsys_state *root_css, *pos;
	struct cgroup *cur_cgrp, *root_cgrp_ref;

	if (!root_cgrp) {
		scx_bpf_error("root_cgrp should not be null");
		goto out;
	}

	struct cgrp_ctx *root_cgrp_ctx;
	if (!(root_cgrp_ctx = lookup_cgrp_ctx(root_cgrp)))
		goto out;

	if (!root_cgrp) {
		scx_bpf_error("root_cgrp should not be null");
		goto out;
	}

	if (!(root_cgrp_ref = bpf_cgroup_acquire(root_cgrp))) {
		scx_bpf_error("Failed to acquire reference to root_cgrp");
		goto out;
	}
	root_css = &root_cgrp_ref->self;

	bpf_rcu_read_lock();
	/*
	 * Iterate over all cgroups, check if any have a cpumask and populate them
	 * as a separate cell.
	 */
	bpf_for_each(css, pos, root_css, BPF_CGROUP_ITER_DESCENDANTS_PRE) {
		cur_cgrp = pos->cgroup;

		/*
		 * We can iterate over dying cgroups, in which case this lookup will
		 * fail. These cgroups can't have tasks in them so just continue.
		 */
		struct cgrp_ctx *cgrp_ctx;
		if (!(cgrp_ctx = lookup_cgrp_ctx_fallible(cur_cgrp)))
			continue;

		int rc = get_cgroup_cpumask(cur_cgrp, entry);
		if (!rc) {
			/*
			 * TODO: If this was a cell owner that just had its cpuset removed,
			 * it should free the cell. Doing so would require draining
			 * in-flight tasks scheduled to the dsq.
			 */
			/* No cpuset, assign to parent cell and continue */
			if (cur_cgrp->kn->id != root_cgid) {
				u32 level = cur_cgrp->level;
				if (level <= 0 || level >= MAX_CG_DEPTH) {
					scx_bpf_error(
						"Cgroup hierarchy is too deep: %d",
						level);
					goto out_rcu_unlock;
				}
				/*
				 * This is a janky way of getting the parent cell, ideally we'd
				 * lookup the parent cgrp_ctx and get it that way, but some
				 * cgroup lookups don't work here because they are (erroneously)
				 * only operating on the cgroup namespace of current. Given this
				 * is a tick() it could be anything. See
				 * https://lore.kernel.org/bpf/20250811175045.1055202-1-memxor@gmail.com/
				 * for details.
				 *
				 * Instead, we just track the parent cells as we walk the cgroup
				 * hierarchy in a separate array. Because the iteration is
				 * pre-order traversal, we're guaranteed to have the current
				 * cgroup's ancestor's cells in level_cells.
				 */
				u32 parent_cell = level_cells[level - 1];
				WRITE_ONCE(cgrp_ctx->cell, parent_cell);
				level_cells[level] = parent_cell;
			}
			continue;
		} else if (rc < 0)
			goto out_rcu_unlock;

		/*
		 * cgroup has a cpumask, allocate a new cell if needed, and assign cpus
		 */
		int cell_idx = READ_ONCE(cgrp_ctx->cell);
		if (!cgrp_ctx->cell_owner) {
			cell_idx = allocate_cell();
			if (cell_idx < 0)
				goto out_rcu_unlock;
			cgrp_ctx->cell_owner = true;
		}

		struct cell_cpumask_wrapper *cell_cpumaskw;
		if (!(cell_cpumaskw =
			      bpf_map_lookup_elem(&cell_cpumasks, &cell_idx))) {
			scx_bpf_error("Failed to find cell cpumask: %d",
				      cell_idx);
			goto out_rcu_unlock;
		}

		struct bpf_cpumask *bpf_cpumask;
		bpf_cpumask = bpf_kptr_xchg(&cell_cpumaskw->tmp_cpumask, NULL);
		if (!bpf_cpumask) {
			scx_bpf_error("tmp_cpumask should never be null");
			goto out_rcu_unlock;
		}
		bpf_cpumask_copy(bpf_cpumask,
				 (const struct cpumask *)&entry->cpumask);
		int cpu_idx;
		bpf_for(cpu_idx, 0, nr_possible_cpus)
		{
			if (bpf_cpumask_test_cpu(
				    cpu_idx,
				    (const struct cpumask *)&entry->cpumask)) {
				struct cpu_ctx *cpu_ctx;
				if (!(cpu_ctx = lookup_cpu_ctx(cpu_idx))) {
					bpf_cpumask_release(bpf_cpumask);
					goto out_rcu_unlock;
				}
				cpu_ctx->cell = cell_idx;
				bpf_cpumask_clear_cpu(cpu_idx,
						      root_bpf_cpumask);
			}
		}
		bpf_cpumask =
			bpf_kptr_xchg(&cell_cpumaskw->cpumask, bpf_cpumask);
		if (!bpf_cpumask) {
			scx_bpf_error("cpumask should never be null");
			goto out_rcu_unlock;
		}

		bpf_cpumask =
			bpf_kptr_xchg(&cell_cpumaskw->tmp_cpumask, bpf_cpumask);
		if (bpf_cpumask) {
			scx_bpf_error("tmp_cpumask should be null");
			bpf_cpumask_release(bpf_cpumask);
			goto out_rcu_unlock;
		}

		barrier();
		WRITE_ONCE(cgrp_ctx->cell, cell_idx);
		u32 level = cur_cgrp->level;
		if (level <= 0 || level >= MAX_CG_DEPTH) {
			scx_bpf_error("Cgroup hierarchy is too deep: %d",
				      level);
			goto out_rcu_unlock;
		}
		level_cells[level] = cell_idx;
	}
	bpf_rcu_read_unlock();

	/*
	 * assign root cell cpus that are left over
	 */
	int cpu_idx;
	bpf_for(cpu_idx, 0, nr_possible_cpus)
	{
		if (bpf_cpumask_test_cpu( cpu_idx, (const struct cpumask *)root_bpf_cpumask)) {
			struct cpu_ctx *cpu_ctx;
			if (!(cpu_ctx = lookup_cpu_ctx(cpu_idx)))
				goto out_root_cgrp;
			cpu_ctx->cell = 0;
		}
	}

	root_bpf_cpumask =
		bpf_kptr_xchg(&root_cell_cpumaskw->cpumask, root_bpf_cpumask);
	if (!root_bpf_cpumask) {
		scx_bpf_error("root cpumask should never be null");
		bpf_cgroup_release(root_cgrp_ref);
		return;
	}

	root_bpf_cpumask = bpf_kptr_xchg(&root_cell_cpumaskw->tmp_cpumask,
					 root_bpf_cpumask);
	if (root_bpf_cpumask) {
		scx_bpf_error("root tmp_cpumask should be null");
		goto out_root_cgrp;
	}

	int cell_idx;
	/* Recalculate L3 counts for all active cells after CPU assignment changes */
	bpf_for(cell_idx, 1, MAX_CELLS) {
		struct cell *cell;
		if (!(cell = lookup_cell(cell_idx))) {
			scx_bpf_error("Lookup for cell %d failed in tick()", cell_idx);
			goto out_root_cgrp;
		}

		if (!cell->in_use)
			continue;

		/* Recalculate L3 counts for each active cell */
		recalc_cell_l3_counts(cell_idx);
	}

	/* Recalculate root cell's L3 counts after cpumask update */
	recalc_cell_l3_counts(ROOT_CELL_ID);

	barrier();
	WRITE_ONCE(applied_configuration_seq, local_configuration_seq);

	bpf_cgroup_release(root_cgrp_ref);
	return;

out_rcu_unlock:
	bpf_rcu_read_unlock();
out_root_cgrp:
	bpf_cgroup_release(root_cgrp_ref);
out:
	if (root_bpf_cpumask)
		bpf_cpumask_release(root_bpf_cpumask);
}

void BPF_STRUCT_OPS(mitosis_running, struct task_struct *p)
{
	struct cpu_ctx *cctx;
	struct task_ctx *tctx;
	struct cell *cell;

	if (!(tctx = lookup_task_ctx(p)) || !(cctx = lookup_cpu_ctx(-1)) ||
	    !(cell = lookup_cell(cctx->cell)))
		return;

	/*
	 * If this task was stolen across L3s, retag to thief L3 and recompute
	 * effective cpumask+DSQ. Preserve vtime to keep fairness.
	 */
#if MITOSIS_ENABLE_STEALING
	if (l3_is_valid(tctx->pending_l3)) {
		u64 save_v = p->scx.dsq_vtime;
		tctx->l3 = tctx->pending_l3;
		tctx->pending_l3 = L3_INVALID;
		update_task_cpumask(p, tctx);
		p->scx.dsq_vtime = save_v;
	}
#endif

	/* Validate task's DSQ before it starts running */
	if (tctx->dsq.raw == DSQ_INVALID) {
		if (tctx->all_cell_cpus_allowed) {
			scx_bpf_error(
				"Task %d has invalid DSQ 0 in running callback (CELL-SCHEDULABLE task, can run on any CPU in cell %d)",
				p->pid, tctx->cell);
		} else {
			scx_bpf_error(
				"Task %d has invalid DSQ 0 in running callback (CORE-PINNED task, restricted to specific CPUs)",
				p->pid);
		}
		return;
	}

	/*
	 * Update per-(cell, L3) vtime for cell-schedulable tasks
	 */
	if (tctx->all_cell_cpus_allowed && l3_is_valid(tctx->l3)) {
		if (time_before(READ_ONCE(cell->l3_vtime_now[tctx->l3]), p->scx.dsq_vtime))
			WRITE_ONCE(cell->l3_vtime_now[tctx->l3], p->scx.dsq_vtime);
	}

	/*
	 * Update CPU vtime for CPU-pinned tasks
	 */
	if (time_before(READ_ONCE(cctx->vtime_now), p->scx.dsq_vtime))
		WRITE_ONCE(cctx->vtime_now, p->scx.dsq_vtime);

	tctx->started_running_at = scx_bpf_now();
}

void BPF_STRUCT_OPS(mitosis_stopping, struct task_struct *p, bool runnable)
{
	struct cpu_ctx *cctx;
	struct task_ctx *tctx;
	struct cell *cell;
	u64 now, used;
	u32 cidx;

	if (!(cctx = lookup_cpu_ctx(-1)) || !(tctx = lookup_task_ctx(p)))
		return;

	cidx = tctx->cell;
	if (!(cell = lookup_cell(cidx)))
		return;

	now = scx_bpf_now();
	used = now - tctx->started_running_at;
	tctx->started_running_at = now;
	/* scale the execution time by the inverse of the weight and charge */
	p->scx.dsq_vtime += used * DEFAULT_WEIGHT_MULTIPLIER / p->scx.weight;

	if (cidx != 0 || tctx->all_cell_cpus_allowed) {
		u64 *cell_cycles = MEMBER_VPTR(cctx->cell_cycles, [cidx]);
		if (!cell_cycles) {
			scx_bpf_error("Cell index is too large: %d", cidx);
			return;
		}
		*cell_cycles += used;

		/*
		 * For cell-schedulable tasks, also accumulate vtime into
		 * per-cell per-L3 queues
		 */
		if (tctx->all_cell_cpus_allowed && l3_is_valid(tctx->l3)) {
			/* Accumulate weighted execution time into per-(cell, L3) vtime */
			cell->l3_vtime_now[tctx->l3] +=
				used * DEFAULT_WEIGHT_MULTIPLIER /
				p->scx.weight;
		}
	}
}

SEC("fentry/cpuset_write_resmask")
int BPF_PROG(fentry_cpuset_write_resmask, struct kernfs_open_file *of,
	     char *buf, size_t nbytes, loff_t off, ssize_t retval)
{
	/*
	 * On a write to cpuset.cpus, we'll need to configure new cells, bump
	 * configuration_seq so tick() does that.
	 */
	__atomic_add_fetch(&configuration_seq, 1, __ATOMIC_RELEASE);
	return 0;
}

s32 BPF_STRUCT_OPS(mitosis_cgroup_init, struct cgroup *cgrp,
		   struct scx_cgroup_init_args *args)
{
	struct cgrp_ctx *cgc;
	if (!(cgc = bpf_cgrp_storage_get(&cgrp_ctxs, cgrp, 0,
					 BPF_LOCAL_STORAGE_GET_F_CREATE))) {
		scx_bpf_error("cgrp_ctx creation failed for cgid %llu",
			      cgrp->kn->id);
		return -ENOENT;
	}

	// Special case for root cell
	if (cgrp->kn->id == root_cgid) {
		WRITE_ONCE(cgc->cell, ROOT_CELL_ID);
		return 0;
	}

	DECLARE_CPUMASK_ENTRY(entry) = allocate_cpumask_entry();
	if (!entry)
		return -EINVAL;
	int rc = get_cgroup_cpumask(cgrp, entry);
	if (rc < 0)
		return rc;
	else if (rc > 0) {
		/*
		 * This cgroup has a cpuset, bump configuration_seq so tick()
		 * configures it.
		 */
		__atomic_add_fetch(&configuration_seq, 1, __ATOMIC_RELEASE);
	}

	/* Initialize to parent's cell */
	struct cgroup *parent_cg;
	if (!(parent_cg = lookup_cgrp_ancestor(cgrp, cgrp->level - 1)))
		return -ENOENT;

	struct cgrp_ctx *parent_cgc;
	if (!(parent_cgc = lookup_cgrp_ctx(parent_cg))) {
		bpf_cgroup_release(parent_cg);
		return -ENOENT;
	}

	bpf_cgroup_release(parent_cg);
	cgc->cell = parent_cgc->cell;
	return 0;
}

s32 BPF_STRUCT_OPS(mitosis_cgroup_exit, struct cgroup *cgrp)
{
	struct cgrp_ctx *cgc;
	if (!(cgc = bpf_cgrp_storage_get(&cgrp_ctxs, cgrp, 0,
					 BPF_LOCAL_STORAGE_GET_F_CREATE))) {
		scx_bpf_error("cgrp_ctx creation failed for cgid %llu",
			      cgrp->kn->id);
		return -ENOENT;
	}

	if (cgc->cell_owner) {
		int ret;
		if ((ret = free_cell(cgc->cell)))
			return ret;
		/*
		 * Need to make sure the cpus of this cell are freed back to the root
		 * cell and the root cell cpumask can be expanded. Bump
		 * configuration_seq so tick() does that.
		 */
		__atomic_add_fetch(&configuration_seq, 1, __ATOMIC_RELEASE);
	}

	return 0;
}

void BPF_STRUCT_OPS(mitosis_cgroup_move, struct task_struct *p,
		    struct cgroup *from, struct cgroup *to)
{
	struct task_ctx *tctx;

	if (!(tctx = lookup_task_ctx(p)))
		return;

	update_task_cell(p, tctx, to);
}

void BPF_STRUCT_OPS(mitosis_set_cpumask, struct task_struct *p,
		    const struct cpumask *cpumask)
{
	struct task_ctx *tctx;

	if (!(tctx = lookup_task_ctx(p)))
		return;

	if (!all_cpumask) {
		scx_bpf_error("NULL all_cpumask");
		return;
	}

	update_task_cpumask(p, tctx);
}

s32 BPF_STRUCT_OPS(mitosis_init_task, struct task_struct *p,
		   struct scx_init_task_args *args)
{
	struct task_ctx *tctx;
	struct bpf_cpumask *cpumask;
	int ret;

	tctx = bpf_task_storage_get(&task_ctxs, p, 0,
				    BPF_LOCAL_STORAGE_GET_F_CREATE);
	if (!tctx) {
		scx_bpf_error("task_ctx allocation failure");
		return -ENOMEM;
	}

	cpumask = bpf_cpumask_create();
	if (!cpumask)
		return -ENOMEM;

	cpumask = bpf_kptr_xchg(&tctx->cpumask, cpumask);
	if (cpumask) {
		/* Should never happen as we just inserted it above. */
		bpf_cpumask_release(cpumask);
		scx_bpf_error("tctx cpumask is unexpectedly populated on init");
		return -EINVAL;
	}

	if (!all_cpumask) {
		scx_bpf_error("missing all_cpumask");
		return -EINVAL;
	}

	/* Initialize L3 to invalid before cell assignment */
	init_task_l3(tctx);

	// TODO clean this up
	if ((ret = update_task_cell(p, tctx, args->cgroup))) {
		return ret;
	}

	return 0;
}

__hidden void dump_cpumask_word(s32 word, const struct cpumask *cpumask)
{
	u32 u, v = 0;

	bpf_for(u, 0, BITS_PER_U32)
	{
		s32 cpu = BITS_PER_U32 * word + u;
		if (cpu < nr_possible_cpus &&
		    bpf_cpumask_test_cpu(cpu, cpumask))
			v |= 1 << u;
	}
	scx_bpf_dump("%08x", v);
}

static void dump_cpumask(const struct cpumask *cpumask)
{
	u32 word, nr_words = (nr_possible_cpus + 31) / 32;

	bpf_for(word, 0, nr_words)
	{
		if (word)
			scx_bpf_dump(",");
		dump_cpumask_word(nr_words - word - 1, cpumask);
	}
}

static void dump_cell_cpumask(int id)
{
	const struct cpumask *cell_cpumask;

	if (!(cell_cpumask = lookup_cell_cpumask(id)))
		return;

	dump_cpumask(cell_cpumask);
}

/* Print cell state for debugging */
static __always_inline void dump_cell_state(u32 cell_idx)
{
	struct cell *cell = lookup_cell(cell_idx);
	if (!cell) {
		scx_bpf_dump("Cell %d: NOT FOUND", cell_idx);
		return;
	}

	scx_bpf_dump("Cell %d: in_use=%d, cpu_cnt=%d, l3_present_cnt=%d",
		   cell_idx, cell->in_use, cell->cpu_cnt, cell->l3_present_cnt);

	u32 l3;
	// TODO Print vtimes for L3s
	// TODO lock
	bpf_for(l3, 0, nr_l3) {
		if (cell->l3_cpu_cnt[l3] > 0) {
			scx_bpf_dump("  L3[%d]: %d CPUs", l3, cell->l3_cpu_cnt[l3]);
		}
	}
}

// TODO: FIX THIS
static __always_inline void dump_l3_state(){
}

void BPF_STRUCT_OPS(mitosis_dump, struct scx_dump_ctx *dctx)
{
	dsq_id_t dsq_id;
	int i;
	struct cell *cell;
	struct cpu_ctx *cpu_ctx;

	scx_bpf_dump_header();

	bpf_for(i, 0, MAX_CELLS)
	{
		if (!(cell = lookup_cell(i)))
			return;

		if (!cell->in_use)
			continue;

		scx_bpf_dump("CELL[%d] CPUS=", i);
		dump_cell_cpumask(i);
		scx_bpf_dump("\n");
		dump_cell_state(i);
	}

	bpf_for(i, 0, nr_possible_cpus)
	{
		if (!(cpu_ctx = lookup_cpu_ctx(i)))
			return;

		dsq_id = get_cpu_dsq_id(i);
		scx_bpf_dump("CPU[%d] cell=%d vtime=%llu nr_queued=%d\n", i,
			     cpu_ctx->cell, READ_ONCE(cpu_ctx->vtime_now),
			     scx_bpf_dsq_nr_queued(dsq_id.raw));
	}

	dump_l3_state();

}

void BPF_STRUCT_OPS(mitosis_dump_task, struct scx_dump_ctx *dctx,
		    struct task_struct *p)
{
	struct task_ctx *tctx;

	if (!(tctx = lookup_task_ctx(p)))
		return;

	scx_bpf_dump(
		"Task[%d] vtime=%llu basis_vtime=%llu cell=%u dsq=%llu all_cell_cpus_allowed=%d\n",
		p->pid, p->scx.dsq_vtime, tctx->basis_vtime, tctx->cell,
		tctx->dsq.raw, tctx->all_cell_cpus_allowed);
	scx_bpf_dump("Task[%d] CPUS=", p->pid);
	dump_cpumask(p->cpus_ptr);
	scx_bpf_dump("\n");
}

s32 BPF_STRUCT_OPS_SLEEPABLE(mitosis_init)
{
	struct bpf_cpumask *cpumask;
	u32 i;
	s32 ret;

	struct cgroup *rootcg;
	if (!(rootcg = bpf_cgroup_from_id(root_cgid)))
		return -ENOENT;

	rootcg = bpf_kptr_xchg(&root_cgrp, rootcg);
	if (rootcg)
		bpf_cgroup_release(rootcg);

	/* setup all_cpumask */
	cpumask = bpf_cpumask_create();
	if (!cpumask)
		return -ENOMEM;

	bpf_for(i, 0, nr_possible_cpus)
	{
		const volatile u8 *u8_ptr;

		if ((u8_ptr = MEMBER_VPTR(all_cpus, [i / 8]))) {
			if (*u8_ptr & (1 << (i % 8))) {
				bpf_cpumask_set_cpu(i, cpumask);
				ret = scx_bpf_create_dsq(get_cpu_dsq_id(i).raw, ANY_NUMA);
				if (ret < 0) {
					bpf_cpumask_release(cpumask);
					return ret;
				}
			}
		} else {
			return -EINVAL;
		}
	}


	cpumask = bpf_kptr_xchg(&all_cpumask, cpumask);
	if (cpumask)
		bpf_cpumask_release(cpumask);

	/* setup cell cpumasks */
	bpf_for(i, 0, MAX_CELLS)
	{
		struct cell_cpumask_wrapper *cpumaskw;
		if (!(cpumaskw = bpf_map_lookup_elem(&cell_cpumasks, &i)))
			return -ENOENT;

		cpumask = bpf_cpumask_create();
		if (!cpumask)
			return -ENOMEM;

		/*
		 * Start with all full cpumask for all cells. They'll get setup in
		 * cgroup_init
		 */
		bpf_cpumask_setall(cpumask);

		cpumask = bpf_kptr_xchg(&cpumaskw->cpumask, cpumask);
		if (cpumask) {
			/* Should be impossible, we just initialized the cell cpumask */
			bpf_cpumask_release(cpumask);
			return -EINVAL;
		}

		cpumask = bpf_cpumask_create();
		if (!cpumask)
			return -ENOMEM;
		cpumask = bpf_kptr_xchg(&cpumaskw->tmp_cpumask, cpumask);
		if (cpumask) {
			/* Should be impossible, we just initialized the cell tmp_cpumask */
			bpf_cpumask_release(cpumask);
			return -EINVAL;
		}
	}

	// cells[0].in_use = true;
	lookup_cell(0)->in_use = true;

	/* Configure root cell (cell 0) topology at init time using nr_l3 and l3_to_cpu masks */
	recalc_cell_l3_counts(ROOT_CELL_ID);

	/* Create (cell,L3) DSQs for all pairs. Userspace will populate maps. */
	// This is a crazy over-estimate
	bpf_for(i, 0, MAX_CELLS)
	{
		u32 l3;
		bpf_for(l3, 0, nr_l3)
		{
			ret = scx_bpf_create_dsq(get_cell_l3_dsq_id(i, l3).raw, ANY_NUMA);
			if (ret < 0)
				scx_bpf_error( "Failed to create DSQ for cell %d, L3 %d: err %d", i, l3, ret);
		}
	}

	return 0;
}

void BPF_STRUCT_OPS(mitosis_exit, struct scx_exit_info *ei)
{
	// int i;
	// bpf_for(i, 0, MAX_CELLS); {
	// 	dump_cell_state((u32)i);
	// }

	UEI_RECORD(uei, ei);
}

SEC(".struct_ops.link")
struct sched_ext_ops mitosis = {
	.select_cpu = (void *)mitosis_select_cpu,
	.enqueue = (void *)mitosis_enqueue,
	.dispatch = (void *)mitosis_dispatch,
	.tick = (void *)mitosis_tick,
	.running = (void *)mitosis_running,
	.stopping = (void *)mitosis_stopping,
	.set_cpumask = (void *)mitosis_set_cpumask,
	.init_task = (void *)mitosis_init_task,
	.cgroup_init = (void *)mitosis_cgroup_init,
	.cgroup_exit = (void *)mitosis_cgroup_exit,
	.cgroup_move = (void *)mitosis_cgroup_move,
	.dump = (void *)mitosis_dump,
	.dump_task = (void *)mitosis_dump_task,
	.init = (void *)mitosis_init,
	.exit = (void *)mitosis_exit,
	.name = "mitosis",
};
# File: scheds/rust/scx_mitosis/src/bpf/mitosis.bpf.h
/* Copyright (c) Meta Platforms, Inc. and affiliates. */
/*
 * This software may be used and distributed according to the terms of the
 * GNU General Public License version 2.
 *
 * This defines the core data structures, types, and constants
 * for the scx_mitosis scheduler, primarily containing `struct cell`
 * and `struct task_ctx`.
 */

#pragma once

#ifdef LSP
#define __bpf__
#include "../../../../include/scx/common.bpf.h"
#include "../../../../include/scx/ravg_impl.bpf.h"
#else
#include <scx/common.bpf.h>
#include <scx/ravg_impl.bpf.h>
#endif

#include "intf.h"

#define MAX_L3S 16

#include "dsq.bpf.h"

/*
 * A couple of tricky things about checking a cgroup's cpumask:
 *
 * First, we need an RCU pointer to pass to cpumask kfuncs. The only way to get
 * this right now is to copy the cpumask to a map entry. Given that cgroup init
 * could be re-entrant we have a few per-cpu entries in a map to make this
 * doable.
 *
 * Second, cpumask can sometimes be stored as an array in-situ or as a pointer
 * and with different lengths. Some bpf_core_type_matches finagling can make
 * this all work.
 */
#define MAX_CPUMASK_ENTRIES (4)

/*
 * We don't know how big struct cpumask is at compile time, so just allocate a
 * large space and check that it is big enough at runtime
 * TODO: This should be deduplicated with the rust code and put in intf.h
 */
#define CPUMASK_LONG_ENTRIES (128)
#define CPUMASK_SIZE (sizeof(long) * CPUMASK_LONG_ENTRIES)

extern const volatile u32 nr_l3;

extern struct cell_map cells;


enum mitosis_constants {

	/* Root cell index */
	ROOT_CELL_ID = 0,

	/* Invalid/unset L3 value */
	// INVALID_L3_ID = -1,

	/* Default weight divisor for vtime calculation */
	DEFAULT_WEIGHT_MULTIPLIER = 100,

	/* Vtime validation multiplier (slice_ns * 8192) */
	VTIME_MAX_FUTURE_MULTIPLIER = 8192,

	/* Bits per u32 for cpumask operations */
	BITS_PER_U32 = 32,

	/* No NUMA constraint for DSQ creation */
	ANY_NUMA = -1,
};

struct cell {
	struct bpf_spin_lock lock;

	// Whether or not the cell is used or not
	u32 in_use;
	// Number of CPUs in this cell
	u32 cpu_cnt;
	// per-L3 vtimes within this cell
	u64 l3_vtime_now[MAX_L3S];
	// Number of CPUs from each L3 assigned to this cell
	u32 l3_cpu_cnt[MAX_L3S];
	// Number of L3s with at least one CPU in this cell
	u32 l3_present_cnt;

	// TODO XXX remove this, only here temporarily to make the code compile
	// current vtime of the cell
	u64 vtime_now;
};

// #if 0
/* Wrap the spin lock in a struct for verifier */
// struct cell_lock_wrapper {
//     struct bpf_spin_lock lock;
// };

// struct cell_locks_map {
//     __uint(type, BPF_MAP_TYPE_ARRAY);
//     __type(key, u32);
//     __type(value, struct cell_lock_wrapper);
//     __uint(max_entries, MAX_CELLS);
// };

#define WITH_CELL_LOCK(cell_ptr, cell_idx, block)                       \
	do {                                                            \
		struct bpf_spin_lock *lock = get_cell_lock(cell_idx);   \
		if (!lock) {                                            \
			scx_bpf_error("Failed to get lock for cell %d", \
				      cell_idx);                        \
			break;                                          \
		}                                                       \
		bpf_spin_lock(lock);                                    \
		block bpf_spin_unlock(lock);                            \
	} while (0)

static inline struct cell *lookup_cell(int idx)
{
	struct cell *cell;

	// cell = MEMBER_VPTR(cells, [idx]);
	cell = bpf_map_lookup_elem(&cells, &idx);


	if (!cell) {
		scx_bpf_error("Invalid cell %d", idx);
		return NULL;
	}
	return cell;
}

static inline struct bpf_spin_lock *get_cell_lock(u32 cell_idx)
{
	if (cell_idx >= MAX_CELLS) {
		scx_bpf_error("Invalid cell index %d", cell_idx);
		return NULL;
	}

	struct cell *cell = lookup_cell(cell_idx);
	if (!cell) {
		scx_bpf_error("Cell %d not found", cell_idx);
		return NULL;
	}
	return &cell->lock;
}
// #endif

/*
 * task_ctx is the per-task information kept by scx_mitosis
 */
struct task_ctx {
	/* cpumask is the set of valid cpus this task can schedule on */
	/* (tasks cpumask anded with its cell cpumask) */
	struct bpf_cpumask __kptr *cpumask;
	/* started_running_at for recording runtime */
	u64 started_running_at;
	u64 basis_vtime;
	/* For the sake of monitoring, each task is owned by a cell */
	u32 cell;
	/* For the sake of scheduling, a task is exclusively owned by either a cell
	 * or a cpu */
	dsq_id_t dsq;
	/* latest configuration that was applied for this task */
	/* (to know if it has to be re-applied) */
	u32 configuration_seq;
	/* Is this task allowed on all cores of its cell? */
	bool all_cell_cpus_allowed;
	// Which L3 this task is assigned to
	s32 l3;

#if MITOSIS_ENABLE_STEALING
	/* When a task is stolen, dispatch() marks the destination L3 here.
	 * running() applies the retag and recomputes cpumask (vtime preserved).
	*/
	s32 pending_l3;
	u32 steal_count; /* how many times this task has been stolen */
	u64 last_stolen_at; /* ns timestamp of the last steal (scx_bpf_now) */
	u32 steals_prevented; /* how many times this task has been prevented from being stolen */
#endif
};

// These could go in mitosis.bpf.h, but we'll cross that bridge when we get
static inline const struct cpumask *lookup_cell_cpumask(int idx);

static inline struct task_ctx *lookup_task_ctx(struct task_struct *p);

/* MAP TYPES */
struct function_counters_map {
	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
	__type(key, u32);
	__type(value, u64);
	__uint(max_entries, NR_COUNTERS);
};

struct cell_map {
	__uint(type, BPF_MAP_TYPE_ARRAY);
	__type(key, u32);
	__type(value, struct cell);
	__uint(max_entries, MAX_CELLS);
};

struct rcu_read_guard {
	bool active;
};

static inline struct rcu_read_guard rcu_read_lock_guard(void)
{
	bpf_rcu_read_lock();
	return (struct rcu_read_guard){ .active = true };
}

static inline void rcu_read_guard_release(struct rcu_read_guard *guard)
{
	if (guard->active) {
		bpf_rcu_read_unlock();
		guard->active = false;
	}
}
#define RCU_READ_GUARD()                                               \
	struct rcu_read_guard __rcu_guard                              \
		__attribute__((__cleanup__(rcu_read_guard_release))) = \
			rcu_read_lock_guard()

struct cpumask_guard {
	struct bpf_cpumask *mask;
};

static inline struct cpumask_guard cpumask_create_guard(void)
{
	struct bpf_cpumask *mask = bpf_cpumask_create();
	return (struct cpumask_guard){ .mask = mask };
}

static inline void cpumask_guard_release(struct cpumask_guard *guard)
{
	if (guard->mask) {
		bpf_cpumask_release(guard->mask);
		guard->mask = NULL;
	}
}

#define CPUMASK_GUARD(var_name)                                       \
	struct cpumask_guard var_name                                 \
		__attribute__((__cleanup__(cpumask_guard_release))) = \
			cpumask_create_guard()
